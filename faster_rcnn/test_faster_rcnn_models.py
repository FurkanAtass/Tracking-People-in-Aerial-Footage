import torch
from torch.utils.data import Dataset, DataLoader
from pycocotools.coco import COCO
import torchvision
from torchvision import transforms
from PIL import Image
import os
from engine import evaluate

TEST_IMAGE_DIR = "../datasets/visdrone/test/images"
TEST_ANNOTATION_PATH = "../datasets/visdrone/visdrone_test.json"
device = torch.device("cuda") if torch.cuda.is_available() else torch.device("mps") if torch.backends.mps.is_available() else torch.device("cpu")

model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, num_classes=2)

model.load_state_dict(torch.load("model_epoch_4.pth", map_location=device))
model.to(device)

test_transform = transforms.Compose([
    transforms.ToTensor()
])
# Load dataset 
class CocoDetectionDataset(Dataset):
    # Init function: loads annotation file and prepares list of image IDs
    def __init__(self, image_dir, annotation_path, transforms=None):
        self.image_dir = image_dir
        self.coco = COCO(annotation_path)
        self.image_ids = list(self.coco.imgs.keys())
        self.transforms = transforms

    # Returns total number of images
    def __len__(self):
        return len(self.image_ids)

    # Fetches a single image and its annotations
    def __getitem__(self, idx):
        image_id = self.image_ids[idx]
        image_info = self.coco.loadImgs(image_id)[0]
        image_path = os.path.join(self.image_dir, image_info['file_name'])
        image = Image.open(image_path).convert("RGB")

        # Load all annotations for this image
        annotation_ids = self.coco.getAnnIds(imgIds=image_id)
        annotations = self.coco.loadAnns(annotation_ids)

        # Extract bounding boxes and labels from annotations
        boxes = []
        labels = []
        for obj in annotations:
            xmin, ymin, width, height = obj['bbox']
            xmax = xmin + width
            ymax = ymin + height
            boxes.append([xmin, ymin, xmax, ymax])
            # Shift label by +1 so 'person' becomes 1 (0 is background)
            labels.append(obj['category_id'] + 1)

        # Convert annotations to PyTorch tensors
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        labels = torch.as_tensor(labels, dtype=torch.int64)
        area = torch.as_tensor([obj['area'] for obj in annotations],    dtype=torch.float32)
        iscrowd = torch.as_tensor([obj.get('iscrowd', 0) for obj in annotations], dtype=torch.int64)

        # Package everything into a target dictionary
        target = {
            "boxes": boxes,
            "labels": labels,
            "image_id": image_id,
            "area": area,
            "iscrowd": iscrowd
        }

        # Apply transforms if any were passed
        if self.transforms:
            image = self.transforms(image)
            # target['boxes'] = self.transforms(target['boxes'])

        return image, target

# Load training dataset
test_dataset = CocoDetectionDataset(
    image_dir=TEST_IMAGE_DIR,
    annotation_path=TEST_ANNOTATION_PATH,
    transforms=test_transform
)

train_loader = DataLoader(test_dataset, batch_size=1, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))
label_list= ["","person"]

model.eval()
results = evaluate(model, train_loader, device=device)
print(results)